{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd7f74e",
   "metadata": {},
   "source": [
    "# bettermdptools experiments demo\n",
    "\n",
    "This notebook demonstrates the **optional** `bettermdptools.experiments.run(...)` entrypoint on a few environments.\n",
    "\n",
    "Notes:\n",
    "- The runs below use **small** iteration and episode counts to keep runtime short.\n",
    "- The purpose is to show the API shape and configuration patterns, not to produce strong policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f10e43f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T13:17:15.223289Z",
     "start_time": "2026-01-07T13:17:14.782680Z"
    }
   },
   "source": [
    "# If you are running this in a fresh environment, you may need:\n",
    "# !pip install bettermdptools\n",
    "\n",
    "from bettermdptools.experiments import run\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "78f12d92",
   "metadata": {},
   "source": [
    "## 1) FrozenLake - value iteration\n",
    "\n",
    "This shows a small value iteration run on a deterministic FrozenLake.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a49e140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T13:17:15.474444Z",
     "start_time": "2026-01-07T13:17:15.407119Z"
    }
   },
   "source": [
    "out_vi = run(\n",
    "    algo=\"vi\",\n",
    "    env_id=\"FrozenLake-v1\",\n",
    "    seed=0,\n",
    "    env_kwargs={\"is_slippery\": False},  # deterministic transitions\n",
    "    algo_kwargs={\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_iters\": 200,   # small for demo\n",
    "        \"theta\": 1e-10,\n",
    "    },\n",
    "    eval_kwargs={\n",
    "        \"n_iters\": 50,    # evaluate for a few episodes\n",
    "        \"render\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"algo:\", out_vi.algo)\n",
    "print(\"env:\", out_vi.env_id)\n",
    "print(\"train keys:\", list(out_vi.train.keys()))\n",
    "print(\"eval keys:\", None if out_vi.eval is None else list(out_vi.eval.keys()))\n",
    "if out_vi.eval and \"scores\" in out_vi.eval:\n",
    "    scores = out_vi.eval[\"scores\"]\n",
    "    print(\"eval scores head:\", scores[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo: vi\n",
      "env: FrozenLake-v1\n",
      "train keys: ['V', 'V_track', 'pi']\n",
      "eval keys: ['scores']\n",
      "eval scores head: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "0a616db7",
   "metadata": {},
   "source": [
    "## 2) FrozenLake - q-learning\n",
    "\n",
    "This uses a small number of episodes, so the learned policy may be weak.\n",
    "The goal is to show how `algo_kwargs` and `eval_kwargs` fit into the call.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "489c0caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T13:17:34.994053Z",
     "start_time": "2026-01-07T13:17:31.761235Z"
    }
   },
   "source": [
    "out_ql = run(\n",
    "    algo=\"q_learning\",   # alias \"q\" is also supported\n",
    "    env_id=\"FrozenLake-v1\",\n",
    "    seed=1,\n",
    "    env_kwargs={\"is_slippery\": False},\n",
    "    algo_kwargs={\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_episodes\": 2000,    # small for demo\n",
    "        \"init_epsilon\": 1.0,\n",
    "        \"min_epsilon\": 0.05,\n",
    "    },\n",
    "    eval_kwargs={\n",
    "        \"n_iters\": 50,\n",
    "        \"render\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"train keys:\", list(out_ql.train.keys()))\n",
    "if \"rewards\" in out_ql.train:\n",
    "    r = out_ql.train[\"rewards\"]\n",
    "    print(\"rewards len:\", len(r), \"last10:\", r[-10:])\n",
    "if out_ql.eval and \"scores\" in out_ql.eval:\n",
    "    scores = out_ql.eval[\"scores\"]\n",
    "    print(\"eval mean:\", sum(scores) / len(scores))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s] 233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train keys: ['Q', 'V', 'pi', 'Q_track', 'pi_track', 'rewards']\n",
      "rewards len: 2000 last10: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "eval mean: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "95ec71b3",
   "metadata": {},
   "source": [
    "## 3) Blackjack - q-learning (very small demo)\n",
    "\n",
    "Blackjack is stochastic and can require many episodes for stable learning.\n",
    "This uses a tiny number of episodes to keep runtime short.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fffbaebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T13:19:15.867068Z",
     "start_time": "2026-01-07T13:19:14.595825Z"
    }
   },
   "source": [
    "out_bj = run(\n",
    "    algo=\"q_learning\",\n",
    "    env_id=\"Blackjack-v1\",\n",
    "    seed=2,\n",
    "    algo_kwargs={\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_episodes\": 5000,   # small for demo, increase for better results\n",
    "        \"min_epsilon\": 0.05,\n",
    "    },\n",
    "    eval_kwargs={\n",
    "        \"n_iters\": 200,\n",
    "        \"render\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"train keys:\", list(out_bj.train.keys()))\n",
    "if out_bj.eval and \"scores\" in out_bj.eval:\n",
    "    scores = out_bj.eval[\"scores\"]\n",
    "    print(\"eval scores head:\", scores[:10])\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train keys: ['Q', 'V', 'pi', 'Q_track', 'pi_track', 'rewards']\n",
      "eval scores head: [ 1. -1.  1. -1.  1. -1. -1. -1. -1.  1.]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "7cfd51b7",
   "metadata": {},
   "source": [
    "## 4) CartPole - discretized q-learning via wrapper registry\n",
    "\n",
    "CartPole is continuous, so planning and tabular RL require discretization.\n",
    "The experiments layer can apply a wrapper (via an internal registry) and pass\n",
    "discretization parameters using `wrapper_kwargs`.\n",
    "\n",
    "This is a short demo run and will not learn a good policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7f540fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T13:20:29.845694Z",
     "start_time": "2026-01-07T13:19:47.555545Z"
    }
   },
   "source": [
    "out_cp = run(\n",
    "    algo=\"q_learning\",\n",
    "    env_id=\"CartPole-v1\",\n",
    "    seed=0,\n",
    "    # wrapper is omitted here to rely on the internal registry for CartPole\n",
    "    wrapper_kwargs={\n",
    "        \"position_bins\": 6,\n",
    "        \"velocity_bins\": 6,\n",
    "        \"angular_velocity_bins\": 6,\n",
    "        \"threshold_bins\": 0.5,\n",
    "        \"angular_center_resolution\": 0.1,\n",
    "        \"angular_outer_resolution\": 0.5,\n",
    "    },\n",
    "    algo_kwargs={\n",
    "        \"gamma\": 0.99,\n",
    "        \"n_episodes\": 1500,   # small for demo\n",
    "        \"min_epsilon\": 0.05,\n",
    "    },\n",
    "    eval_kwargs={\n",
    "        \"n_iters\": 20,\n",
    "        \"render\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"meta:\", out_cp.meta)\n",
    "print(\"train keys:\", list(out_cp.train.keys()))\n",
    "if \"rewards\" in out_cp.train:\n",
    "    r = out_cp.train[\"rewards\"]\n",
    "    print(\"rewards len:\", len(r), \"last10:\", r[-10:])\n",
    "if out_cp.eval and \"scores\" in out_cp.eval:\n",
    "    scores = out_cp.eval[\"scores\"]\n",
    "    print(\"eval scores:\", scores)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 622/1500 [00:07<00:21, 40.63it/s] 177: UserWarning: Episode was truncated.  TD target value may be incorrect.\n",
      "  warnings.warn(\n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta: {'env': {'source': 'wrapped', 'wrapped': True, 'wrapper': 'CartpoleWrapper'}}\n",
      "train keys: ['Q', 'V', 'pi', 'Q_track', 'pi_track', 'rewards']\n",
      "rewards len: 1500 last10: [194. 159. 273. 268. 278. 111. 195. 206. 187. 190.]\n",
      "eval scores: [269. 166. 206. 264. 222. 153. 162. 179. 220. 238. 230. 236. 234. 241.\n",
      " 224. 184. 222. 228. 144. 231.]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d27bebe35a292db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
